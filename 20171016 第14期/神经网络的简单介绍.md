### A Quick Introduction to Neural Networks

原文链接：[A Quick Introduction to Neural Networks](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?from=hackcv&hmsr=hackcv.com&utm_medium=hackcv.com&utm_source=hackcv.com)

人工神经网，是受人脑中神经元处理信息的方式激励，通过使用电脑建立的模拟这种数据处理方式的建立起来的模型。由于在语义理解、机器视觉和文本处理方向的突破性成就，人工神经网络令机器学习领域和工业界同仁都感到十分激动。在这篇博文中，我们简单的理解一种比较特殊的人工神经网络——多层感知机。

#### **一个单独的神经元**

在神经网络中最基础的计算单元就是**神经元**，经常被叫**节点**或者**单元**。其他节点或者外部数据由它输入，经过计算输出。每个输入有一个与之对应的**权重**W。同时这个节点经过下面的**激活方程**作用到权重矩阵和输入矩阵的乘积之和。

![Screen Shot 2016-08-09 at 3.42.21 AM.png](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-3-42-21-am.png?w=568&h=303)

###### Figure 1: a single neuron

上面的网络需要数值的输入**X1**和**X2**权重**W1**和**W2**分别相乘，另外需要加上一个称为偏置的**b**。以后我们将了解到更多关于偏置的细节信息。

图1中从神经元中计算得到的输出**Y**。方程**F**是非线性的一般称为**激活函数**。激活函数的目的就是引入非线性到神经元的输出。这一点的重要性表现在，现实生活中的数据一般都是非线性的，我们需要这些神经元学习表现非线性关系。

每一种激活函数使数值产生固定的数学运算。下面介绍几种经常用到的激活函数。

- **Sigmoid**激活函数: 接收一个实数值输入值，将值转化到0-1区间之内。

σ(x) = 1 / (1 + exp(−x))

- **tanh**激活函数**:** 接收一个实数值输入值，将值转化到[-1, 1]区间之内。

tanh(x) = 2σ(2x) − 1

- **ReLU**激活函数: ReLU代表一个修正线性单元。接收一个实数值输入值，当值大于零保持不变，当小于零输出值为零。

f(x) = max(0, x)

下图展示以上激活函数：

###### ![Screen Shot 2016-08-08 at 11.53.41 AM](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-08-at-11-53-41-am.png?w=748)Figure 2: different activation functions

**偏置的重要性**: 偏置的主要功能在于给每个节点提供一个常数值（作为节点常规输入值的补充）。 See [this link](http://stackoverflow.com/q/2480650/3297280) to learn more about the role of bias in a neuron.

#### **前馈神经网络**

前馈神经网络是最早的最简单的人工神经网络的设计。它包含神经元在**层**间的你安排，相邻的层之间有有**连接**或者**边**。所有这些连接都有相应的**权重值**。

以下是一个三层神经网络图：

![Screen Shot 2016-08-09 at 4.19.50 AM.png](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-4-19-50-am.png?w=498&h=368)

###### Figure 3: an example of feedforward neural network

一个前馈神经网络可以包含三种类型的神经元：

 

\1.         **输入神经元**--输入神经元给神经网络带来外部信息，被称为输入层。在输入层没有涉及计算--它仅仅是将信息传入隐含层。

\2.         **隐含层神经元**--隐含层的神经元与玩不神经之间没有之间联系，因此称为隐含层。他们能计算和传递输入层与输出层之间的信息。全部的隐含层神经元组成“隐含层”这个概念。一般一个前馈神经网络有一层输入层和一层单独的输出层，可以没有或者有多层隐藏层。

\3.         **输出神经元**--输出神经元集体被称为输出层，代表着从网络到外部的计算和传导。

在一个前馈神经网络中，信息只在一个方向运行-前向-由输入层，经由隐藏层到达输出层。在前馈神经网络中没有循环或者圆环网络（这种特性使得前馈神经网络不通与循环神经网络）。

下面是两个前馈神经网络的例子：

\1.         **单层感知机**--这是最简单的前馈神经网，这种网络不包含隐藏层。

\2.         **多层感知机-**-一个多层感知机有一层或者多层隐藏层。由于实际应用中，相对于单层感知机而言，多层感知机更实用，接下来我们将介绍多层感知机。

**多层感知机**

一个多层感知机（MLP）除了输出层和输出层之外，包含一层或者多层隐藏层。一般单层感知机只能训练线性模型，而多层感知机不仅可以训练线性模型，同时可以训练非线性摩西。

图4 展示了一个有一层隐藏层的多层感知机。需要注意的是所有连接均包含与之相对应的权重，只是在图中标示出了w0,w1,w3三个权重。

**输入层**：输入层有三个神经元。偏置神神经元值设置值为1，其他两个神经元分别作为外部输入设置为x1,x2（根据输入值情况数值型标示）。跟上面介绍的一样，在输入层没有计算操作，所以经过层数输出进入隐藏层的分别是1，x1,x2。

**隐藏层**：隐藏层同样有三个神经元，其中偏置神经元的输出设置为1。其他两个神经元的输出取决于输入层的的输出1,x1,x2和连接边的权重的乘积。图中标红的神经元，展示这个神经元的计算方式，另一个神经元以同样的方式输出。需要注意的是其中f指代的是激活函数。这些输出将会输入到输出层。

![ds.png](https://ujwlkarn.files.wordpress.com/2016/08/ds.png?w=1128)

###### Figure 4: a multi layer perceptron having one hidden layer

**输出层**：输出层有两个神经元，它们接收隐含层的输入，执行类似标红神经元的操作。计算出的Y1 Y2就是这个多层感知机的结果。

在给定一组特征x=(x1,x2,...)和目标变量y的前提下，一个多层感知机能学习特征和目标变量之间的关系，进而建立分类或者回归模型。

下面使用一个例子来更好的理解多层感知机。假设我们有一下学生的标记数据：

![train.png](https://ujwlkarn.files.wordpress.com/2016/08/train.png?w=297&h=112)

两个输入项分别表明学生学习的时间和在期中考中获得的成绩。结果列有两个值0或者1，分别表明学生在期末考中是否能通过。例如，我们可知如果学生学习35个小时，在期中考试中获得70分，那么他或者她最终能通过期末考。

现在，结社，我们需要预测一个学习学习了25个小时期中考70分能否通过最终考试。

![test.png](https://ujwlkarn.files.wordpress.com/2016/08/test.png?w=300&h=40)

这是一个多层感知机能通过已知例子学习，并给出预测的，二分类的问题。接下来饿哦们看看多层感知机是如何学习这些关系的。

训练我们自己的多层感知机：反向传播算法

训练多层感知机的过程被称为后向传播。我建议读下下面索引中的内容，这些内容给后向传播比较明确的介绍。

**误差反向传播**，也被简写为反向传播，是人工神经网络训练的一种方式。是监督学习的一种，是需要通过标记过的数据来训练模型的。

简单来说，后向传播类似**从错误中学习**。这种监督意味着修正网络产生的错误。

人工神经网络包含来自不同层的神经元：谁层、中间的隐藏层、和输出层。相邻层之间通过与权重相乘来连接。学习的目标给边分配对的权重。在输入矩阵已知的情况下，这些权重决定输出矩阵。

在有监督学习中，训练集是有标记的。这就意味着，对于给定的输出，我们知道想要的输出值或者标签。

**后向传播**：随机的初始化分配边上的权重值。对于训练集中的每次输入，人工神经网络均被激活同时它的产出也是可以被观察到的。网络的输出将同已知标记好的产出进行比较，差异将通过后向传播传给之前的层。权重会根据误差相应的调整。这个过程将不断进行直到输出的错误率小于事先设置的门槛值。

一旦上面的进程终止，我们训练完的人工神经网络。

既然我们队后向传播有了大体理解，现在我们回到上面学生分数的数据集。

 

图5中展示的多层感知机在输出层除了偏置神经元，有两个神经元接收输入的学习时间和期中成绩。感知机中隐藏层，除了偏置神经元之外有两个神经元。输出层也有两个神经元-上面点神经元输出通过的概率，下面点的神经元输出挂掉的概率。

 

在分类任务中，我们一般使用归一化指数函数在输出层作为激活函数，来保证输出概率和为1.归一化指数函数可以将一个任意真实值转化到0-1之间，并保证和为1.所以，在这个例子中，p(pass)+p(fail)=1

 

**第1步：前向过程**

随机初始化网络中的权重。让我们只观看下图5中被标记为V的神经元。假设相关的权重分别为w1,w2,w3。

网络接到第一个训练集中的样本作为输入。

l  输入=【35,67】

l  标记的结果=【1,0】

经过下面的计算神经元V的输出：

V = **f** (1*w1 + 35*w2 + 67*w3)

类似地，隐藏层中另一个神经元也将被计算出来。隐含层中的神经元经过类似输出层神经元类似的操作输入到输出层。这使得我们能够计算输出层输出的概率。

 

假设，输出层两个神经元给出的概率分别为0.4和0.6（因为权值是随机初始化的，所以输出也是随机的）。这里我们可以发现计算出的概率同想要的概率之间纯真很大差异。因此，图5中的网络输出了一个错误输出。

![Screen Shot 2016-08-09 at 11.52.57 PM.png](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-11-52-57-pm.png?w=748)

###### Figure 5: forward propagation step in a multi layer perceptron

**第2****步：后向传播和权重更新**

我们计算了输出神经元中的误差，同时后向传播通过这些误差计算梯度。以减少输出层的误差为目标，使用类似梯度下降的优化器，来调整网络中的全部参数。这个过程展示在下图6中。

假设与神经元相关的新权证是w4,w5,w6（后向传播和调整权重后）。

![Screen Shot 2016-08-09 at 11.53.06 PM.png](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-11-53-06-pm.png?w=748)

###### Figure 6: backward propagation and weight updation step in a multi layer perceptron

如果我们现在输入相同的样本带网络中，网络应该表现的更好一点，因为我们已经通过最小化误差调整了权重参数。正如图7中所示，输入层的误差同【0.6，-0.4】相比，已经减少到【0.2，-0.2】。这就意味着网络已经学会了如何正确的分类我们之前的训练样本。

![Screen Shot 2016-08-09 at 11.53.15 PM.png](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-11-53-15-pm.png?w=748)

###### Figure 7: the MLP network now performs better on the same input

我们在所有的训练样本中重复以上过程。我们的网络就能学习这些样本。

 

假设我们想预测一个学生学习了25个小时期中考试75分是否能通过期末考，我们将数据输入到网络中就能得到结果。

 

这里我尽量避免使用数学上方程和解释梯度下降的概念，想通过建立一种对这种算法直观的理解。

 

三维视角的多层感知机

Adam Harley建立一种三维视角的剁成感知机，这种感知机已经在手写字识别中被训练和运用。

 

该网络将784个数字像素值作为来自手写数字的28×28图像的输入（其在输入层中具有对应于像素的784个节点）。网络在第一个隐藏层中有300个节点，在第二个隐藏层中有100个节点，在输出层中有10个节点（对应于10个数字）

虽然与前面部分讨论的网络相比，此处描述的网络要大得多（使用更多的隐藏层和节点），但前向传播步骤和反向传播步骤中的所有计算都以相同的方式（在每个节点处）完成，如上所述之前。

![Screen Shot 2016-08-09 at 5.45.34 PM.png](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-5-45-34-pm.png?w=748)

###### Figure 8: visualizing the network for an input of ‘5’

具有比其他输出值更高的输出值的节点由更亮的颜色表示。在输入层中，亮节点是接收较高数值像素值作为输入的节点。请注意，在输出层中，唯一的明亮节点对应于数字5（输出概率为1，高于其他9个输出概率为0的节点）。这表明MLP已正确分类输入数字。我强烈建议玩这种可视化并观察不同层节点之间的连接。

#### 深度神经网络

1. \1.      [深度学习和普通机器学习有什么区别？](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md)

   \2.      [神经网络和深层神经网络有什么区别？](http://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network?rq=1)

   \3.         [深层学习与多层感知器有何不同？](https://www.quora.com/How-is-deep-learning-different-from-multilayer-perceptron)


#### 结论

我已经跳过了本文中讨论的一些概念的重要细节，以便于理解。我会建议通过去[第一部分](http://cs231n.github.io/neural-networks-1/)，[第2部分](http://cs231n.github.io/neural-networks-2/)，[第三部分](http://cs231n.github.io/neural-networks-3/)和[案例分析](http://cs231n.github.io/neural-networks-case-study/)从斯坦福大学的神经网络教程多层感知的全面理解。

#### References

1. [Artificial Neuron Models](https://www.willamette.edu/~gorr/classes/cs449/ann-overview.html)
2. [Neural Networks Part 1: Setting up the Architecture (Stanford CNN Tutorial)](http://cs231n.github.io/neural-networks-1/)
3. [Wikipedia article on Feed Forward Neural Network](https://en.wikipedia.org/wiki/Feedforward_neural_network)
4. [Wikipedia article on Perceptron ](https://en.wikipedia.org/wiki/Perceptron)
5. [Single-layer Neural Networks (Perceptrons) ](http://computing.dcu.ie/~humphrys/Notes/Neural/single.neural.html)
6. [Single Layer Perceptrons ](http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/2-Perceptrons.pdf)
7. [Weighted Networks – The Perceptron](http://page.mi.fu-berlin.de/rojas/neural/chapter/K3.pdf)
8. [Neural network models (supervised) (scikit learn documentation)](http://scikit-learn.org/dev/modules/neural_networks_supervised.html)
9. [What does the hidden layer in a neural network compute?](http://stats.stackexchange.com/a/63163/53914)
10. [How to choose the number of hidden layers and nodes in a feedforward neural network? ](http://stats.stackexchange.com/a/1097/53914)
11. [Crash Introduction to Artificial Neural Networks](http://ulcar.uml.edu/~iag/CS/Intro-to-ANN.html)
12. [Why the BIAS is necessary in ANN? Should we have separate BIAS for each layer?](http://stackoverflow.com/questions/7175099/why-the-bias-is-necessary-in-ann-should-we-have-separate-bias-for-each-layer)
13. [Basic Neural Network Tutorial – Theory](https://takinginitiative.wordpress.com/2008/04/03/basic-neural-network-tutorial-theory/)
14. [Neural Networks Demystified (Video Series): Part 1, Welch Labs @ MLconf SF](https://www.youtube.com/watch?v=5MXp9UUkSmc)
15. A. W. Harley, “An Interactive Node-Link Visualization of Convolutional Neural Networks,” in ISVC, pages 867-877, 2015 ([link](http://scs.ryerson.ca/~aharley/vis/harley_vis_isvc15.pdf))