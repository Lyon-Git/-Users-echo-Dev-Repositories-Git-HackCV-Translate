# 信息理论概论和你应该关心的原因

原文链接：[信息理论概论和你应该关注的原因](https://recast.ai/blog/introduction-information-theory-care/?from=hackcv&hmsr=hackcv.com&utm_medium=hackcv.com&utm_source=hackcv.com)

我们来谈谈信息 - 它是什么？我们可以多大程度的压缩它？信息沟通有哪些限制？我们将通过观察一群人 - 克劳德·香农（Claude E. Shannon）以及跟随他的脚步的男人和女人的革命性工作来回答这些问题。最重要的是，我们将尝试回答“这对机器学习至关重要吗？为什么？”。我希望当你读完这篇文章时，你也会相信信息理论对任何对开发机器学习系统感兴趣的人都非常有益。让我们首先讨论香农1948年工作之前理解“信息”这个词的一般方式。然后我们将准备好理解这项工作的革命性质。

# **信息论简史**

在20世纪上半叶，世界迅速通过不同类型的**模拟通信**变得越来越紧密。这包括公共无线电广播以及双向无线电通信（例如船舶或飞机）。当时缺乏对“信息”一词的理解，每种类型的数据（如语音，图片，电影等）的传播都是基于其自身的理论和实践。然而，一般的理解是：对于任何给定的通信信道，通信具有两个特征 - 速率（在给定时间段内可以传输的数据量）和可靠性。增加其中的每一个都会以牺牲另一个为代价。当计算机科学开始成为一个领域时，一般的理解是这种权衡在这种情况下也将保持正确。（注意：在这篇文章中，我选择通过将信息理论视为数字通信科学来简化问题，

考虑以下通道，称为BEC（二进制擦除通道）：每个位在通过通道传输时，可以在另一侧无错接收（概率为1-p）或被擦除（概率为p）：

 

[![img](https://recast.ai/blog/wp-content/uploads/2017/09/BEC-e1506504023977.png)](https://recast.ai/blog/wp-content/uploads/2017/09/BEC.png)

*二进制擦除通道（BEC）。每个比特以概率p擦除*

假设我们想要传输比特“0”。如果我们发送一次，那么很有可能（p）它将无法生存。我们可以尝试发送两次，并降低概率![P 1 2](https://s0.wp.com/latex.php?latex=p%5E2&bg=ffffff&fg=000000&s=0)，但我们将付出沉重的代价 - 我们必须使用两次通道才能传输一位信息。当然，我们可以通过一次又一次地发送相同的比特来继续降低擦除的可能性，但是费率的价格会继续增加。Shannon表明这种权衡可以被打破，我们稍后会回到BEC，看看如何。

让我们现在开始探索香农的结果和信息理论。要理解这篇文章中的数学，你需要概率论的一些基本概念，但不要太担心，我会直观地解释一切。

# **基本概念**

## **重要物理量及其含义**

大多数机器学习从业者已经使用了信息理论中的一些概念，有时甚至不知道它。以下是您可能已经熟悉的一些基本数量。

### **熵**

通常称为![H（X）](https://s0.wp.com/latex.php?latex=H%28X%29&bg=ffffff&fg=000000&s=0)随机变量X的熵 可以通过计算得到![H（X）=  -  \ sum \ limits_ {x \ in \ mathcal {X}} P_X（x）\ log P_X（x）](https://s0.wp.com/latex.php?latex=H%28X%29+%3D+-+%5Csum%5Climits_%7Bx+%5Cin+%5Cmathcal%7BX%7D%7D+P_X%28x%29%5Clog+P_X%28x%29&bg=ffffff&fg=000000&s=0)。 熵可以被认为是变量X中  固有的“混乱”的度量- 给定字母表的大小（X  可以采用![X = x](https://s0.wp.com/latex.php?latex=X+%3D+x&bg=ffffff&fg=000000&s=0)的不同值的数量），字母表上的均匀分布使熵最大化，而已知value（）  给出![H（X）= 0](https://s0.wp.com/latex.php?latex=H%28X%29+%3D+0&bg=ffffff&fg=000000&s=0)。例如，考虑伯纳利分布，在大小为2的字母表中定义：![P_X（0）= p，\ quad P_X（1）= 1-p](https://s0.wp.com/latex.php?latex=P_X%280%29+%3D+p%2C+%5Cquad+P_X%281%29+%3D+1-p&bg=ffffff&fg=000000&s=0)。 熵最大化![p = \ frac {1} {2}](https://s0.wp.com/latex.php?latex=p+%3D+%5Cfrac%7B1%7D%7B2%7D&bg=ffffff&fg=000000&s=0) （“均匀”分布）并且最小化![p = 0](https://s0.wp.com/latex.php?latex=p+%3D+0&bg=ffffff&fg=000000&s=0)或![p = 1](https://s0.wp.com/latex.php?latex=p+%3D+1&bg=ffffff&fg=000000&s=0) （确定性）。另请注意，对于离散随机变量，熵不能为负数：

 

[![img](https://recast.ai/blog/wp-content/uploads/2017/09/Binary_entropy_plot.png)](https://recast.ai/blog/wp-content/uploads/2017/09/Binary_entropy_plot.png)

*伯努利随机变量X的熵，作为X的概率为1的函数*

 

有时我们想要测量一些信息已经知道后留下的“混乱”。为了做到这一点，我们可以使用熵的条件版本，定义如下：![H(X | Y)=  -  \ sum \ limits _ {(x，y)\ in \ mathcal {X} \ times \ mathcal {Y}} P_ {XY}(x，y)\ log P_ {X | Y} (X | Y)](https://s0.wp.com/latex.php?latex=H%28X%7CY%29+%3D+-+%5Csum%5Climits_%7B%28x%2Cy%29+%5Cin+%5Cmathcal%7BX%7D%5Ctimes%5Cmathcal%7BY%7D%7D+P_%7BXY%7D%28x%2Cy%29+%5Clog+P_%7BX%7CY%7D%28x%7Cy%29&bg=ffffff&fg=000000&s=0)。在这里，Y  代表我们已经知道的信息。但请注意，在计算中我们不假设Y 的实际值，而是用它的平均值：![H(X | Y)= \ mathbb {E} [H(X | Y = y)]](https://s0.wp.com/latex.php?latex=H%28X%7CY%29+%3D+%5Cmathbb%7BE%7D%5BH%28X%7CY%3Dy%29%5D&bg=ffffff&fg=000000&s=0)。

### **相互信息**

互信息是在两个随机变量上定义的度量。它有助于我们深入了解一个数据（或一个随机变量）携带的信息。看一下互信息的数学定义：![I（X; Y）= \ sum \ limits _ {（x，y）\ in \ mathcal {X} \ times \ mathcal {Y}} P_ {XY}（x，y）\ log \ frac {P_ {XY} （X，Y）} {P_X（x）的P_Y（Y）}](https://s0.wp.com/latex.php?latex=I%28X%3BY%29+%3D+%5Csum%5Climits_%7B%28x%2Cy%29+%5Cin+%5Cmathcal%7BX%7D%5Ctimes%5Cmathcal%7BY%7D%7D+P_%7BXY%7D%28x%2Cy%29+%5Clog+%5Cfrac%7BP_%7BXY%7D%28x%2Cy%29%7D%7BP_X%28x%29P_Y%28y%29%7D&bg=ffffff&fg=000000&s=0)可以看出，实际上它让我们对“ X  和Y 彼此独立![I（X; Y）= H（X） -  H（X | Y）= H（Y） -  H（Y | X）](https://s0.wp.com/latex.php?latex=I%28X%3BY%29+%3D+H%28X%29+-+H%28X%7CY%29+%3D+H%28Y%29+-+H%28Y%7CX%29&bg=ffffff&fg=000000&s=0)多远？” 这一问题的答案有所了解。通过一些简单的代数可以证明。换句话说，互信息是X中固有的“混乱”与知道Y  后X中剩下的“混乱” 之间的差异。（当变量改变角色时也是如此）。请注意，虽然两个变量的相互信息及其相关性都给出了关于它们之间关系的提示，但它们从两个非常不同的角度来看问题。这是不同二维散点图和相互信息的相应值：

[![img](https://recast.ai/blog/wp-content/uploads/2017/09/Mutual_Information_Examples.svg_.png)](https://recast.ai/blog/wp-content/uploads/2017/09/Mutual_Information_Examples.svg_.png)

*不同2变量概率分布的互信息值。在每个子图中，散点表示变量上的分布，由X轴和Y轴（源）表示。*

您可以考虑每个图片的X 轴和Y 轴的相关性，并亲眼看看相关和互信息的行为有很大不同（例如：窄直线的轴之间的相关性是什么？一个圆圈？你可以到[这里](https://commons.wikimedia.org/wiki/File:Correlation_examples.png)找出来。）

### **KL分歧**

当我们要比较两个概率分布，![P_X](https://s0.wp.com/latex.php?latex=P_X&bg=ffffff&fg=000000&s=0)并且![Q_X](https://s0.wp.com/latex.php?latex=Q_X&bg=ffffff&fg=000000&s=0)，这样做的一个方式是通过库尔贝克-莱布勒（KL）散度（有时被称为KL距离，虽然它不是一个数学距离）![\ mathcal {D}（P_X || Q_X）= \ sum \ limits_ {x \ in \ mathcal {X}} P_X（x）\ log \ frac {P_X（x）} {Q_X（x）}](https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D%28P_X%7C%7CQ_X%29+%3D+%5Csum%5Climits_%7Bx+%5Cin+%5Cmathcal%7BX%7D%7D+P_X%28x%29+%5Clog+%5Cfrac%7BP_X%28x%29%7D%7BQ_X%28x%29%7D&bg=ffffff&fg=000000&s=0)。在许多情况下，这个数量非常有用。例如，它构成了在尝试对源进行编码时支付代码长度的代价，![X \ sim P_X](https://s0.wp.com/latex.php?latex=X+%5Csim+P_X&bg=ffffff&fg=000000&s=0) 就好像它是根据分发一样![Q_X](https://s0.wp.com/latex.php?latex=Q_X&bg=ffffff&fg=000000&s=0)。吉隆坡分歧如此有用的原因之一是财产![\ mathcal {D}（P_X || Q_X）\ geq 0](https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D%28P_X%7C%7CQ_X%29+%5Cgeq+0&bg=ffffff&fg=000000&s=0)，当且仅当相等时才具有平等性![P_X \ equiv Q_X](https://s0.wp.com/latex.php?latex=P_X+%5Cequiv+Q_X&bg=ffffff&fg=000000&s=0)。注^ h H但是，那![\ mathcal {D}（P_X || Q_X）\ neq \ mathcal {D}（Q_X || P_X）](https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D%28P_X%7C%7CQ_X%29+%5Cneq+%5Cmathcal%7BD%7D%28Q_X%7C%7CP_X%29&bg=ffffff&fg=000000&s=0)，这就是为什么KL信息量不是数学距离。此外，互信息实际上只是KL分歧的一个私人案例，我们在那里比较联合分布X  和Y 及其产品分布：![I（X; Y）= \ mathcal {D}（P_ {XY} || P_XP_Y）](https://s0.wp.com/latex.php?latex=I%28X%3BY%29+%3D+%5Cmathcal%7BD%7D%28P_%7BXY%7D%7C%7CP_XP_Y%29&bg=ffffff&fg=000000&s=0)。

如果您有兴趣深入了解KL分歧，以及上面提到的其他数量等，请查看[此博客文章](http://colah.github.io/posts/2015-09-Visual-Information/)。它包含整洁的可视化，并解释了所有这些与编码理论的关系。

## **让我们玩**

现在我们已经了解了信息理论中最基本的三个数量，让我们对它们有一些乐趣！以下是一些需要记住的重要概念：

### **调节减少熵**

![H（X | Y）\ leq H（X）](https://s0.wp.com/latex.php?latex=H%28X%7CY%29+%5Cleq+H%28X%29&bg=ffffff&fg=000000&s=0)。这个概念是非常直观-在“大锅饭” X 时Ÿ 知道不能比的“烂摊子”做大X 不知道ÿ 。如果是这样的话，我们总是可以“忽略”我们对Y 的了解！至于证明，我们已经拥有了所有这些工具。之间的相互信息X  和Ÿ  只是之间的KL散度![P_ {} XY](https://s0.wp.com/latex.php?latex=P_%7BXY%7D&bg=ffffff&fg=000000&s=0)和![P_XP_Y](https://s0.wp.com/latex.php?latex=P_XP_Y&bg=ffffff&fg=000000&s=0)。因此，![I（X; Y）= H（X） -  H（X | Y）\ geq 0](https://s0.wp.com/latex.php?latex=I%28X%3BY%29+%3D+H%28X%29+-+H%28X%7CY%29+%5Cgeq+0&bg=ffffff&fg=000000&s=0) 因此![H（X）\ geq H（X | Y）](https://s0.wp.com/latex.php?latex=H%28X%29+%5Cgeq+H%28X%7CY%29&bg=ffffff&fg=000000&s=0)。

### **连锁规则**

通过基本的对数属性很容易证明链式规则 - 试试吧！它告诉我们X  和Y  中的联合“混乱” 正好等于X  中的“混乱” ，除了Y中的混乱，当X  已经知道时：![H（X，Y）= H（X）+ H（Y | X）](https://s0.wp.com/latex.php?latex=H%28X%2CY%29+%3D+H%28X%29+%2B+H%28Y%7CX%29&bg=ffffff&fg=000000&s=0)。

### **数据处理定理**

如果你只记得这篇博文中的一件事，我希望就是这样。让我们从数学公式开始，稍后留下解释：

让我们![X \ leftrightarrow Y \ leftrightarrow Z.](https://s0.wp.com/latex.php?latex=X+%5Cleftrightarrow+Y+%5Cleftrightarrow+Z&bg=ffffff&fg=000000&s=0) 按顺序形成马尔可夫链。然后![I（X; Z）\ leq \ min [I（X; Y），I（Y; Z）]](https://s0.wp.com/latex.php?latex=I%28X%3BZ%29+%5Cleq+%5Cmin+%5BI%28X%3BY%29%2C+I%28Y%3BZ%29%5D&bg=ffffff&fg=000000&s=0)。

但是，这是什么意思？![X \ leftrightarrow Y \ leftrightarrow Z.](https://s0.wp.com/latex.php?latex=X+%5Cleftrightarrow+Y+%5Cleftrightarrow+Z&bg=ffffff&fg=000000&s=0) 如果Y  是X  和Z 之间唯一的“连接” ，我们说形成马尔可夫链。换句话说，给定Y ，知道Z  也不提供关于X的任何附加信息。在这种情况下，X  和Z  之间的互信息小于![I（X; Y）](https://s0.wp.com/latex.php?latex=I%28X%3BY%29&bg=ffffff&fg=000000&s=0)和![我（Y; Z）](https://s0.wp.com/latex.php?latex=I%28Y%3BZ%29&bg=ffffff&fg=000000&s=0)。这个定理的一个有趣的私人案例是要考虑![X = f（Y）](https://s0.wp.com/latex.php?latex=X+%3D+f%28Y%29&bg=ffffff&fg=000000&s=0)。在这种情况下，![X \ leftrightarrow Y \ leftrightarrow Z.](https://s0.wp.com/latex.php?latex=X+%5Cleftrightarrow+Y+%5Cleftrightarrow+Z&bg=ffffff&fg=000000&s=0) 确实形成马尔可夫链，其重要性在于通过处理数据，我们永远无法获得有关隐藏质量的信息。例如，假设Y. 表示数据集中的可能图片，Z  表示图片中的主要对象（Z  的字母表因此是“猫”，“狗”，“球”，“汽车”等）。通过处理Y  （例如通过卷积神经网络），**不可能**创建关于Z的新信息，我们所希望的是尽可能少地丢失。你可能会问，卷积神经网络如何运作得如此之好？这是因为他们可以从图片中**提取**重要信息以进行分类，但他们永远无法**创建**它。

### **渐近均分属性**

有人可能会争辩说，这个属性是单独负责使信息理论发生魔力。它告诉我们，给定一系列独立且相同分布（iid）的实验![X_1，X_2，\ ldots X_n \ sim P_X（x）](https://s0.wp.com/latex.php?latex=X_1%2C+X_2%2C+%5Cldots+X_n+%5Csim+P_X%28x%29&bg=ffffff&fg=000000&s=0)，他们的经验熵将非常接近理论熵![P_X](https://s0.wp.com/latex.php?latex=P_X&bg=ffffff&fg=000000&s=0):( ![-  \ frac {1} {n} \ log P_X（X_1，X_2，\ ldots X_n）\到H（X）](https://s0.wp.com/latex.php?latex=-%5Cfrac%7B1%7D%7Bn%7D%5Clog+P_X%28X_1%2CX_2%2C+%5Cldots+X_n%29+%5Cto+H%28X%29&bg=ffffff&fg=000000&s=0) 概率）。这意味着，鉴于大量此类实验，所有概率将仅在相对较小的序列集之间划分，我们称之为**典型**序列。让我们举一个例子：想象一个**非公平的**硬币，以0.7的概率落在头上。投掷这个硬币100次，显然任何特定序列的概率非常小。尽管如此，几乎所有的概率都在一组结果中，这些结果具有大约（但不一定完全）70个头和30个尾部。如果我们继续投掷硬币1000次，那么仅包含大约700个头的结果的概率将更接近1.实现1000个这样的实验的小蟒蛇代码，每个实验有1000个硬币投掷，只有一个实验结果在650到750之间的许多头中：

```
import random

experiments = 1000 # number of complete experiments
toss_per_experiment = 1000  #number of tosses per experiment
delta = 50 # the tolerance
prob_heads = 0.7 # the probability the coin lands on heads
count = 0

for exper in range(experiments):
  heads = 0
  for toss in range(toss_per_experiment):
    if random.random() <= prob_heads:
      heads += 1
  if (heads <= toss_per_experiment * prob_heads - delta) or (heads >= toss_per_experiment * prob_heads + delta):
    count += 1

print(count)
```

我邀请你自己玩参数 - 尝试分别改变实验次数，每次实验的投掷和容差'delta'，看看会发生什么。

# **容量**

Shannon从1948年开始的原始论文中充满了重要而有趣的结果（请自己看看，写得非常愉快。请参阅本文末尾的完整引文和链接）。可以说，本文中两个最重要的结果可以称为**源编码定理**和**信道编码定理**。

## **源编码定理**

Ñ 随机变量![X_1，X_2，\ ldots，X_n](https://s0.wp.com/latex.php?latex=X_1%2C+X_2%2C+%5Cldots%2C+X_n&bg=ffffff&fg=000000&s=0)，所有独立地被分布![P_X（X）](https://s0.wp.com/latex.php?latex=P_X%28x%29&bg=ffffff&fg=000000&s=0)，可以压缩成任何数目的比特是严格大于![NH（X）](https://s0.wp.com/latex.php?latex=nH%28X%29&bg=ffffff&fg=000000&s=0) 与信息损失的风险可忽略![n \ to \ infty](https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&bg=ffffff&fg=000000&s=0)。

但是，从理论的角度来看，如何实现这一目标呢？嗯，渐近均分属性告诉我们所有的概率都在典型的集合中。因此，将代码中的代码字分发给**不在**此集合中的![2 ^ {NH（X）}](https://s0.wp.com/latex.php?latex=2%5E%7BnH%28X%29%7D&bg=ffffff&fg=000000&s=0) 序列是没有意义的。典型套装的尺寸是多少？它包含有关序列的内容，因此有意义的是![NH（X）](https://s0.wp.com/latex.php?latex=nH%28X%29&bg=ffffff&fg=000000&s=0) ，为了表示这些序列，我们不需要超过一些位，而且它们只是！

这个结果让我们回到关于熵性质的直观解释：对于每个随机变量![X_1，X_2，\ ldots，X_n](https://s0.wp.com/latex.php?latex=X_1%2C+X_2%2C+%5Cldots%2C+X_n&bg=ffffff&fg=000000&s=0)，  其中固有的“混乱”是![H（X）](https://s0.wp.com/latex.php?latex=H%28X%29&bg=ffffff&fg=000000&s=0)。那么，有意义的是，为了压缩每个“实验”的结果，我们需要做的就是消除固有的“混乱”吗？但请注意，这仅适用于n  非常大的限制- 如果我们只想压缩一些实验的结果，我们可能需要更多资源。

## **信道编码定理**

有噪声的信道编码定理表明任何通信信道都具有容量 - 可以在信道上可靠地传输的最大通信速率（例如，每个信道使用的比特数）（错误概率低至我们想要的低）成为！）。**任何频道**都是如此，无论它有多嘈杂。当然，如果信道根本不让信号通过，则容量为零。让我们举一个简单的例子：考虑我们已经看到的**二进制擦除通道**（BEC）：

[![img](https://recast.ai/blog/wp-content/uploads/2017/09/BEC-e1506504023977.png)](https://recast.ai/blog/wp-content/uploads/2017/09/BEC.png)

*二进制擦除通道。尽管有擦除，但在信道容量下可以进行可靠的通信*

显然，发送n比特（假设n很大），我们可以假设![N(1-p)的](https://s0.wp.com/latex.php?latex=n%281-p%29&bg=ffffff&fg=000000&s=0) 这些比特中的大约会安全到达。不幸的是，如果我们选择这种“盲目”策略，我们无法预先知道哪些比特会被丢弃。令人惊讶的是，这个频道的容量实际上是![1-P](https://s0.wp.com/latex.php?latex=1-p&bg=ffffff&fg=000000&s=0)！ 这意味着，如果对于n通道使用，我们愿意仅仅传输![N(1-p)的](https://s0.wp.com/latex.php?latex=n%281-p%29&bg=ffffff&fg=000000&s=0) 信息比特而不是n比特，我们可以物理地“推”到通道中，我们可以保证这些![N(1-p)的](https://s0.wp.com/latex.php?latex=n%281-p%29&bg=ffffff&fg=000000&s=0) 位将安全到达另一侧！然而，重要的是要记住信息量（以比特为单位）和信道上的比特数之间的巨大差异，这等于信道使用的数量 - 我们仍将在信道上发送n比特。通信，但嵌入其中的信息只相当于![N(1-p)的](https://s0.wp.com/latex.php?latex=n%281-p%29&bg=ffffff&fg=000000&s=0) 比特。![N(1-p)的](https://s0.wp.com/latex.php?latex=n%281-p%29&bg=ffffff&fg=000000&s=0) 然而，这些信息将安全地到达另一方。

# **机器学习中的信息理论（或：我为什么要关心？）**

恭喜你一直在这里！我希望我能够以直观的方式成功地传达这些原则，而且我所做的数学运算必须包含在内。你来到这里的奖励是找出 - 为什么所有这些对于机器学习都很重要？

在其他任何事情之前，在我看来，对于了解一点信息理论所带来的概念的直觉和基本理解是进入机器学习世界的最有价值的教训。将任何ML问题视为从输入处的原始数据点到输出处的答案创建**通道**的问题，对于该问题，**信息**它传达了数据所需的属性最大化，可以让你从不同的角度看待许多不同的问题。上面提到的数量在许多不同的情况下也是有用的。例如，考虑我们想要在相同数据上比较两个无监督聚类算法的情况，以便检查它们是否给出相似结果。为什么不使用每个算法的结果之间的互信息，其中X是随机变量，表示第一个算法为任何数据点选择的簇，Y代表第二个算法？另一种选择是使用条件熵（![H（X | Y）](https://s0.wp.com/latex.php?latex=H%28X%7CY%29&bg=ffffff&fg=000000&s=0)或![H（Y | X）](https://s0.wp.com/latex.php?latex=H%28Y%7CX%29&bg=ffffff&fg=000000&s=0)）当用一种算法猜测聚类结果时，看到剩下多少“乱七八糟”，而另一种算法聚类的结果已经知道（这两种比较方法非常相似，但有区别，你能不能发现它？）。

在本节的其余部分，让我们试着仔细研究ML中的一些有趣结果，这是与信息理论联系的直接结果：

## **培训决策树**

用于训练决策树的最流行的算法之一基于最大化**信息增益**的原理。虽然给出了不同的名称，但与每个特征相对应的信息增益恰好是该特征与数据点标签之间的相互信息（您可以[在此处查看](https://en.wikipedia.org/wiki/Decision_tree_learning)）。这实际上很有意义 - 当试图确定哪个是分割树的最佳功能时，为什么不选择提供结果最多信息的那个？换句话说，为什么不选择在分裂之后尽可能多地消散“混乱”的那个呢？拆分后会发生什么？我们如何继续建造树木？拆分后的每个节点都可以用新的随机变量表示，我们可以为每个结果节点再次启动整个过程。

考虑到这个过程，我们也可以对另一个非常重要的问题 - 正规化 - 有所了解。决策树是需要最多正规化的模型之一，因为经验告诉我们几乎每次都会完全“自由”树木。让我们通过数据处理定理来考虑这个问题：我们希望做的是增加数据点的实际标签之间的互信息（让我们称之为标签Z以保持与上面的数据处理定理一致）和预测标签X不幸的是，为了预测标签，我们只能使用属性Y.我们通过增加X和Y之间的互信息来尽力而为，但根据定理，这并不能保证增加![I（X; Z）](https://s0.wp.com/latex.php?latex=I%28X%3BZ%29&bg=ffffff&fg=000000&s=0)。![I（X; Y）](https://s0.wp.com/latex.php?latex=I%28X%3BY%29&bg=ffffff&fg=000000&s=0)你可能还记得，只构成**上界**了![I（X; Z）](https://s0.wp.com/latex.php?latex=I%28X%3BZ%29&bg=ffffff&fg=000000&s=0)。因此，在第一步中，信息增益是显着的，并且很有可能它至少部分地促进了增益![I（X; Z）](https://s0.wp.com/latex.php?latex=I%28X%3BZ%29&bg=ffffff&fg=000000&s=0)，这是我们真正想要的。随着树的增长，信息增益变得越来越不重要，增加的希望![I（X; Z）](https://s0.wp.com/latex.php?latex=I%28X%3BZ%29&bg=ffffff&fg=000000&s=0) 减少了，而我们得到的只是过度拟合数据。因此，当信息增益变得无关紧要时，最好截断树。

## **通过压缩进行聚类**

另一个有趣的例子是通过压缩进行聚类。这里的主要思想是使用流行的压缩算法，例如负责ZIP，RAR，Gif等的压缩算法，这些算法对于具有重复特征的文件具有良好的性能，以便确定数据点属于哪个类别。我们通过将新数据点附加到表示类的每个文件，并选择在压缩数据点时性能最高的文件来实现。

这种方法利用了Lempel Ziv（LZ）系列压缩算法（例如参见[此处](https://en.wikipedia.org/wiki/LZ77_and_LZ78)）。虽然这些算法有许多不同的变化，但主要思想保持不变：从头到尾压缩文档，在每个点上，已知段落通过对先前外观的引用进行编码，同时添加全新信息“词典”，以便在再次遇到类似的段时可用。LZ系列算法的成员之间创建和管理此字典的方式可能不同，但主要思想保持不变。

使用这种类型的算法来压缩文件，很明显，从这种类型的压缩中受益最多的文档类型是**长而重复的文档。**这是因为源编码定理告诉我们无损压缩是从下面开始的![NH（X）](https://s0.wp.com/latex.php?latex=nH%28X%29&bg=ffffff&fg=000000&s=0) ，并且表示混乱的熵对于随机文件比对重复文件要大得多。对于这些重复文档，算法将有足够的“时间”来学习它们中的模式，然后一次又一次地使用它们以便以有效的方式保存信息。

事实证明，文件中的相似性可以用于良好压缩的事实可以用于分类。对于监督分类（类存在且包含大量数据作为“示例”），可以将新数据点附加到任何现有文件（其中每个文件代表“类”），以及声明的类数据点是相对于其原始大小（以位为单位）成功压缩数据点的数据点。请注意，由于我们将数据点附加到每个文档的末尾，因此当压缩算法到达新数据点时，现有文档中的所有信息应该已经存在于相应的“词典”中。因此，如果任何现有文档与新数据点之间存在相似性，

考虑到无监督分类（或**聚类**），类似的方法仍然有用。使用不同数据点组合的联合压缩的“成功”水平，可以创建聚类，例如每个类中的数据点一起压缩得很好。当然，在这种情况下，有一些问题需要解决，主要有与大量组合的测试和最终的聚类算法的复杂性做的，但这些问题是可以解决的，如在例如做[这个非常完成工作](https://arxiv.org/abs/cs/0312044)。这种聚类方法的优点是不需要预先定义要使用的特征。以音乐文件聚类的问题为例，其他方法要求我们首先定义和提取文件的不同特征，例如节拍，音高，艺术家姓名等。在这里，我们需要做的就是检查哪些文件压缩得很好。然而，重要的是要记住**无免费午餐引理**：这里的整个魔法都包含在压缩过程中，因此了解所选择的具体压缩算法势在必行。字典是如何构建的？怎么用？什么是“足够长”的文件要被它压缩？这些特性可以确定压缩易于使用的相似性类型，从而确定控制聚类的特征。

## [![🤖](https://s.w.org/images/core/emoji/11/svg/1f916.svg) 快乐机器人大楼 ![🤖](https://s.w.org/images/core/emoji/11/svg/1f916.svg)](https://recast.ai/blog/build-your-first-bot-with-recast-ai/?utm_source=blog&utm_medium=article)

# **我在哪里可以了解更多？**

信息理论世界是巨大的，需要学习很多东西。如果这篇文章给你这样做的愿望（我希望它这样做），这里有一些好的开始：

如果你对Claude E. Shannon的传记和成就感兴趣，我邀请你看看这篇博文，我觉得这篇文章非常有趣：[https](https://www.technologyreview.com/s/401112/claude-shannon-reluctant-father-of-the-digital-age/)：[//www.technologyreview.com/s/401112/claude -shannon-不愿意，父亲的最数字时代/](https://www.technologyreview.com/s/401112/claude-shannon-reluctant-father-of-the-digital-age/)

此外，您可以在这里找到新出版的香农传记：[https](https://www.amazon.com/Mind-Play-Shannon-Invented-Information/dp/1476766681/ref=sr_1_1?ie=UTF8&qid=1503066163&sr=8-1&keywords=a+mind+at+play)：[//www.amazon.com/Mind-Play-Shannon-Invented-Information/dp/1476766681/ref=sr_1_1？ie = UTF8＆qid = 1503066163＆sr = 8- 1个关键字+游戏= A +心态+](https://www.amazon.com/Mind-Play-Shannon-Invented-Information/dp/1476766681/ref=sr_1_1?ie=UTF8&qid=1503066163&sr=8-1&keywords=a+mind+at+play)

如果您感兴趣的是信息理论本身，我邀请您从Shannon的原始论文开始，该论文非常全面且易于阅读：

- CE Shannon，“通信的数学理论”，载于*“贝尔系统技术期刊”*，第一卷。27，不。3，第379-423，1948年七月<http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf>

此外，还有一些书籍也是一个很好的起点。据我所知，大多数学生从其中一个开始：

- Cover，TM和Thomas，JA（2012）。*信息论的要素*。John Wiley＆Sons。
- 加拉格，RG（1968）。*信息理论和可靠的沟通*（第2卷）。纽约：威利。

一本有点难以开始的书但可能值得为它的数学完整性而工作的是以下一本：

- Csiszar，I。，＆Körner，J。（2011）。*信息论：离散无记忆系统的编码定理*。剑桥大学出版社。

如果您对Lempel-Ziv压缩特别感兴趣，可以从很多论文开始。试试这个：

- Ziv，J。和Lempel，A。（1977）。用于顺序数据压缩的通用算法。*IEEE信息理论学报*，*23*（3），337-343。

其他资源包含在这篇文章的正文中，当然，信息理论世界也有丰富的有趣结果，你可以很容易地找到你的心愿。玩得开心，谢谢你的阅读。

#### 想建立自己的会话机器人？开始使用Recast.AI！