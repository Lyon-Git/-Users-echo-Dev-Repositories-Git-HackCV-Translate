# 深度学习的局限性
原文链接：[The limitations of deep learning](https://blog.keras.io/the-limitations-of-deep-learning.html)

本章节改编自我一本书的第九章第2部分，[Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff) (Manning Publications)

[![Deep learning with Python](https://blog.keras.io/img/deep_learning_with_python_cover_thumbnail.png)](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff)

这是一系列两篇关于当前深度学习局限性和未来发展文章的一部分。

本文适合已经拥有重要深度学习经验的人（比如说那些已经阅读了本书一到八章节的人），我们假定你已经拥有许多前置知识。

# 深度学习：几何视图
最令人惊讶的事情是深度学习如此简单。十年前，没有人期望我们能够通过使用简单的、用梯度下降训练的参数模型，在机器感知问题方面实现如此惊人的成果。现在，你只需要有足够大量的参数模型，并用足够多的示例进行梯度下降训练。就如Feynman曾经谈论过宇宙，“它并不复杂，它只是很多”。

在深度学习里，一切事物都是一个向量，即一切事物都是几何空间中的一个点。模型输入（它可以是文本、图片等等）和目标首先会被向量化，即转化成一些初始的输入向量空间和目标向量空间。当数据经过深度学习中的每个层面，都会做一次简单的几何变换。总之，模型的层链形成了一个非常复杂的几何变换，它们可以分解为一系列的简单几何变换。这个复杂的转换企图将输入空间映射到目标空间，一次一个点。这个转换被每个层面的权重参数化，这些权重基于模型当前表现的良好程度进行更新。这种几何变换的一个关键特性是它必须是可微分的，目的是让我们能够通过梯度下降训练来学习它的参数。直观地说，这意味着从输入到输出的几何变形必须是平滑而且连续的——这是一个重要的约束。

对输入数据应用复杂的几何变换的这整个过程可以被3D可视化，通过想象一个尝试将皱纸球解皱的人：皱纸球是模型开始时输入数据的歧管，在纸球上被人操作的每一个动作类似于被一个层面操作的一个简单几何变换，这整个解皱手势序列就是整个模型的复杂变换。深度学习模型是用于解开复杂的高维数据流形的数学机器。

这就是深度学习的神奇之处：把含义转换成向量、几何空间，然后学习复杂的几何变换，将一个空间映射到另一个空间。你需要的仅仅是足够高的维度，用于捕捉在原始数据中找到的关系的全部范围。


# 深度学习的局限性
用这种简单的策略实现的应用空间几乎是无穷的，然而，更多的应用是现在深度学习技术不能实现的——就算是给出了大量的人工注释的数据。例如，假设您可以组装一个数十万甚至数百万个软件产品功能的英语描述的数据集，由产品经理编写，以及由团队工程师开发的相应源代码来满足这些需求。即使有了这些数据，您也不能通过简单地训练一个深度学习模型来读取产品描述并生成适当的代码库。这只是许多例子中的一个例子。一般来说，任何需要推理（如编程）、应用科学方法（如长期规划）或者类似算法的数据操纵的东西，对于深度学习模型来说都是遥不可及的，不管你向它扔了多少数据。即使学习具有深度学习网络的排序算法也是相当困难的。

这是因为深度学习模型只是通过一系列简单的、连续的几何变换将一个向量映射到另一个向量空间。假设存在从X到Y到可学习的连续变换，以及可用作训练数据的X:Y的密集采样，它所能做的就是将一个数据流形X映射到另一个流形Y。因此，即使深度学习模型可以理解为一种程序，但是，大多数程序不能用深度学习模型所表达——这是对于大多数任务而言，或者不存在相应的解决该任务的实际大小的深层神经网络，或者即使存在一种情况，它可能也不是可学习的，即相应的几何变换可能太复杂，或者可能没有合适的数据可以用来学习它。

通过堆叠更多的层和使用更多的训练数据来扩展当前的深度学习技术，只能从表面上缓解这些问题。它不能解决更基本的问题，即深层学习模型在表示内容方面非常有限，并且人们希望能够学习的大多数程序不能表示为数据流形的连续几何变形。

# 机器学习拟人化的风险

当代人工智能的一个非常真实的风险是误解深度学习模型的作用，并且高估了它们的能力。人类头脑的一个基本特征是"头脑理论"，即我们倾向于投射关于我们周围事物的意图、信仰和知识。在岩石上画一张笑脸使它"快乐"在我们心中。例如，应用于深度学习，这意味着当我们能够稍微成功地训练一个模型来生成描述图片的字幕时，我们被引导相信该模型"理解"图片的内容及它所生成的字幕。然后，当训练数据中出现的图像稍有偏离导致模型开始生成完全荒谬的字幕时，我们就会感到非常惊讶。

