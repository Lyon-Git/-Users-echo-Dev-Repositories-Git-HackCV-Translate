# 深度学习的局限性
原文链接：[The limitations of deep learning](https://blog.keras.io/the-limitations-of-deep-learning.html)

本章节改编自我一本书的第九章第2部分，[Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff) (Manning Publications)
[![Deep learning with Python](https://blog.keras.io/img/deep_learning_with_python_cover_thumbnail.png)](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff)

这是一系列两篇关于当前深度学习局限性和未来发展文章的一部分。

本文适合已经拥有重要深度学习经验的人（比如说那些已经阅读了本书一到八章节的人），我们假定你已经拥有许多前置知识。

# 深度学习：几何视图
最令人惊讶的事情是深度学习如此简单。十年前，没有人期望我们能够通过使用简单的、用梯度下降训练的参数模型，在机器感知问题方面实现如此惊人的成果。现在，你只需要有足够大量的参数模型，并用足够多的示例进行梯度下降训练。就如Feynman曾经谈论过宇宙，“它并不复杂，它只是很多”。

在深度学习里，一切事物都是一个向量，即一切事物都是几何空间中的一个点。模型输入（它可以是文本、图片等等）和目标首先会被向量化，即转化成一些初始的输入向量空间和目标向量空间。当数据经过深度学习中的每个层面，都会做一次简单的几何变换。总之，模型的层链形成了一个非常复杂的几何变换，它们可以分解为一系列的简单几何变换。这个复杂的转换企图将输入空间映射到目标空间，一次一个点。这个转换被每个层面的权重参数化，这些权重基于模型当前表现的良好程度进行更新。这种几何变换的一个关键特性是它必须是可微分的，目的是让我们能够通过梯度下降训练来学习它的参数。直观地说，这意味着从输入到输出的几何变形必须是平滑而且连续的——这是一个重要的约束。

对输入数据应用复杂的几何变换的这整个过程可以被3D可视化，通过想象一个尝试将皱纸球解皱的人：皱纸球是模型开始时输入数据的歧管，在纸球上被人操作的每一个动作类似于被一个层面操作的一个简单几何变换，这整个解皱手势序列就是整个模型的复杂变换。深度学习模型是用于解开复杂的高维数据流形的数学机器。

这就是深度学习的神奇之处：把含义转换成向量、几何空间，然后学习复杂的几何变换，将一个空间映射到另一个空间。你需要的仅仅是足够高的维度，用于捕捉在原始数据中找到的关系的全部范围。

